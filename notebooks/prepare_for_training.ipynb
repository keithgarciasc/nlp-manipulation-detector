{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d82b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import & Setup\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e92b01e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 3092\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>manipulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$200,000 worth of supplies donated to PB schools</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10-year-old shoot mom over VR headset</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14 killed in prison clashes in Ecuador as some...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025 The New York Times Company</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23-year-old charged with murder in NH country ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  manipulative\n",
       "0   $200,000 worth of supplies donated to PB schools             0\n",
       "1              10-year-old shoot mom over VR headset             0\n",
       "2  14 killed in prison clashes in Ecuador as some...             0\n",
       "3                    2025 The New York Times Company             0\n",
       "4  23-year-old charged with murder in NH country ...             0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and merge all datasets\n",
    "\n",
    "location_relative_path = \"../data/processed/cleaned/\"\n",
    "\n",
    "# Load each dataset\n",
    "df = pd.read_csv(location_relative_path + \"all_label.txt\", sep=\"|\", skiprows=1, names=[\"text\", \"manipulative\"])\n",
    "#df2 = pd.read_csv(location_relative_path + \"liar_train.txt\", sep=\"|\", skiprows=1, names=[\"text\", \"manipulative\"])\n",
    "#df3 = pd.read_csv(location_relative_path + \"mentalmanip_con.txt\", sep=\"|\", skiprows=1, names=[\"text\", \"manipulative\"])\n",
    "#df4 = pd.read_csv(location_relative_path + \"tweets.txt\", sep=\"|\", skiprows=1, names=[\"text\", \"manipulative\"])\n",
    "\n",
    "# Merge all into one DataFrame\n",
    "#df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "# Convert label column to integer\n",
    "df['manipulative'] = df['manipulative'].astype(int)\n",
    "\n",
    "# Preview the first few rows\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b66f55fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 3092\n",
      "Training samples: 2473\n",
      "Testing samples: 619\n",
      "\n",
      "Sample training texts and labels:\n",
      "Text: Bloomberg Intelligence: TikTok Algorithm to Be Secured (Podcast)\n",
      "Manipulative: 0\n",
      "---\n",
      "Text: News outlets 'desperately concerned' for their journalists in Gaza, urge Israeli authorities to help\n",
      "Manipulative: 1\n",
      "---\n",
      "Text: Your favorite clothing brand accused of using forced labor overseas  \n",
      "Manipulative: 1\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'].tolist(), df['manipulative'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Testing samples: {len(test_texts)}\")\n",
    "\n",
    "# Preview a few training examples\n",
    "print(\"\\nSample training texts and labels:\")\n",
    "for i in range(3):\n",
    "    print(f\"Text: {train_texts[i]}\")\n",
    "    print(f\"Manipulative: {train_labels[i]}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e9278a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training encodings keys: ['input_ids', 'attention_mask']\n",
      "Test encodings keys: ['input_ids', 'attention_mask']\n",
      "\n",
      "Sample tokenized training input_ids:\n",
      "Sample 1: [101, 22950, 4454, 1024, 14841, 25509, 6559, 9896, 2000, 2022]\n",
      "Sample 2: [101, 2739, 11730, 1005, 9652, 4986, 1005, 2005, 2037, 8845]\n",
      "Sample 3: [101, 2115, 5440, 5929, 4435, 5496, 1997, 2478, 3140, 4450]\n",
      "\n",
      "Sample attention masks:\n",
      "Sample 1: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Sample 2: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Sample 3: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize training and test texts\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Training encodings keys: {list(train_encodings.keys())}\")\n",
    "print(f\"Test encodings keys: {list(test_encodings.keys())}\")\n",
    "\n",
    "# Preview a few training samples\n",
    "print(\"\\nSample tokenized training input_ids:\")\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}: {train_encodings['input_ids'][i][:10]}\")\n",
    "\n",
    "print(\"\\nSample attention masks:\")\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}: {train_encodings['attention_mask'][i][:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4271e676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 2473\n",
      "Test dataset size: 619\n",
      "\n",
      "Sample training items:\n",
      "input_ids: torch.Size([53]) | tensor([  101, 22950,  4454,  1024, 14841, 25509,  6559,  9896,  2000,  2022])\n",
      "attention_mask: torch.Size([53]) | tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "labels: 0\n",
      "---\n",
      "input_ids: torch.Size([53]) | tensor([  101,  2739, 11730,  1005,  9652,  4986,  1005,  2005,  2037,  8845])\n",
      "attention_mask: torch.Size([53]) | tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "labels: 1\n",
      "---\n",
      "input_ids: torch.Size([53]) | tensor([ 101, 2115, 5440, 5929, 4435, 5496, 1997, 2478, 3140, 4450])\n",
      "attention_mask: torch.Size([53]) | tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "labels: 1\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#Wrap as Torch Dataset\n",
    "class ManipulationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Instantiate datasets\n",
    "train_dataset = ManipulationDataset(train_encodings, train_labels)\n",
    "test_dataset = ManipulationDataset(test_encodings, test_labels)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Preview a few samples from train_dataset\n",
    "print(\"\\nSample training items:\")\n",
    "for i in range(3):\n",
    "    sample = train_dataset[i]\n",
    "    for key, val in sample.items():\n",
    "        if val.dim() > 0:\n",
    "            print(f\"{key}: {val.shape} | {val[:10]}\")\n",
    "        else:\n",
    "            print(f\"{key}: {val.item()}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f779060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Optional) Save Datasets\n",
    "\n",
    "tokenized_relative_path = f\"../data/processed/tokenized/\"\n",
    "\n",
    "torch.save(train_dataset, tokenized_relative_path + \"train_dataset.pt\")\n",
    "torch.save(test_dataset, tokenized_relative_path + \"test_dataset.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6bcd3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([53]) | tensor([  101, 22950,  4454,  1024, 14841, 25509,  6559,  9896,  2000,  2022])\n",
      "attention_mask: torch.Size([53]) | tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "labels: 0\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "loaded_test_dataset = torch.load(tokenized_relative_path + \"train_dataset.pt\", weights_only=False)\n",
    "\n",
    "# View a sample\n",
    "sample = loaded_test_dataset[0]\n",
    "for key, val in sample.items():\n",
    "    if val.dim() > 0:\n",
    "        print(f\"{key}: {val.shape} | {val[:10]}\")\n",
    "    else:\n",
    "        print(f\"{key}: {val.item()}\")  # For scalar tensors like labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
