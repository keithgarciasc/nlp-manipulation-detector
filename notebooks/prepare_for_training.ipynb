{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d82b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import & Setup\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92b01e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and merge all datasets\n",
    "\n",
    "location_relative_path = \"../data/processed/cleaned/\"\n",
    "\n",
    "# Load each dataset\n",
    "df1 = pd.read_csv(location_relative_path + \"clickbait_data.txt\", sep=\"|\", skiprows=1, names=[\"text\", \"manipulative\"])\n",
    "df2 = pd.read_csv(location_relative_path + \"liar_train.txt\", sep=\"|\", skiprows=1, names=[\"text\", \"manipulative\"])\n",
    "df3 = pd.read_csv(location_relative_path + \"mentalmanip_con.txt\", sep=\"|\", skiprows=1, names=[\"text\", \"manipulative\"])\n",
    "df4 = pd.read_csv(location_relative_path + \"tweets.txt\", sep=\"|\", skiprows=1, names=[\"text\", \"manipulative\"])\n",
    "\n",
    "# Merge all into one DataFrame\n",
    "df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "# Convert label column to integer\n",
    "df['manipulative'] = df['manipulative'].astype(int)\n",
    "\n",
    "# Preview the first few rows\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66f55fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'].tolist(), df['manipulative'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Testing samples: {len(test_texts)}\")\n",
    "\n",
    "# Preview a few training examples\n",
    "print(\"\\nSample training texts and labels:\")\n",
    "for i in range(3):\n",
    "    print(f\"Text: {train_texts[i]}\")\n",
    "    print(f\"Manipulative: {train_labels[i]}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize training and test texts\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Training encodings keys: {list(train_encodings.keys())}\")\n",
    "print(f\"Test encodings keys: {list(test_encodings.keys())}\")\n",
    "\n",
    "# Preview a few training samples\n",
    "print(\"\\nSample tokenized training input_ids:\")\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}: {train_encodings['input_ids'][i][:10]}\")\n",
    "\n",
    "print(\"\\nSample attention masks:\")\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}: {train_encodings['attention_mask'][i][:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4271e676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrap as Torch Dataset\n",
    "class ManipulationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Instantiate datasets\n",
    "train_dataset = ManipulationDataset(train_encodings, train_labels)\n",
    "test_dataset = ManipulationDataset(test_encodings, test_labels)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Preview a few samples from train_dataset\n",
    "print(\"\\nSample training items:\")\n",
    "for i in range(3):\n",
    "    sample = train_dataset[i]\n",
    "    for key, val in sample.items():\n",
    "        if val.dim() > 0:\n",
    "            print(f\"{key}: {val.shape} | {val[:10]}\")\n",
    "        else:\n",
    "            print(f\"{key}: {val.item()}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f779060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Optional) Save Datasets\n",
    "\n",
    "tokenized_relative_path = f\"../data/processed/tokenized/\"\n",
    "\n",
    "torch.save(train_dataset, tokenized_relative_path + \"train_dataset.pt\")\n",
    "torch.save(test_dataset, tokenized_relative_path + \"test_dataset.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bcd3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "loaded_test_dataset = torch.load(tokenized_relative_path + \"train_dataset.pt\", weights_only=False)\n",
    "\n",
    "# View a sample\n",
    "sample = loaded_test_dataset[0]\n",
    "for key, val in sample.items():\n",
    "    if val.dim() > 0:\n",
    "        print(f\"{key}: {val.shape} | {val[:10]}\")\n",
    "    else:\n",
    "        print(f\"{key}: {val.item()}\")  # For scalar tensors like labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
